import numpy as np
import math

class MLP_Complete:

    """
    Constructor: Computes MLP.

    Args:
        inputLayer (int): size of input
        hiddenLayers (array-like): number of layers and size of each layers.
        hiddenLayer (int): size of hidden layer.
        outputLayer (int): size of output layer
        seed (scalar): seed of the random numeric.
        epislom (scalar) : random initialization range. e.j: 1 = [-1..1], 2 = [-2,2]...
    """

    def __init__(self,inputLayer,hiddenLayers, outputLayer,seed=0, epislom = 0.12):
        
        np.random.seed(seed)
        
        self.inputLayer = inputLayer
        self.hiddenLayers = hiddenLayers
        self.outputLayer = outputLayer
        self.epsilom = epislom
        
        # [] lo convierte a lista de un elemento
        # list convierte el array a lista
        self.layer_sizes = [inputLayer]     # Ponemos la capa de input
        for i in hiddenLayers:              # Ponemos todas las capas ocultas
            self.layer_sizes += [i]
        self.layer_sizes += [outputLayer]   # Ponemos la capa del output

        self.num_layers = len(self.layer_sizes)

        # Inicializamos los theta aleatoriamente
        # Habrá tantas thetas como capas haya - 1
        self.thetas = []
        self.new_trained(self.thetas, self.epsilom)

        """
    Reset the theta matrix created in the constructor by both theta matrix manualy loaded.

    Args:
        theta1 (array_like): Weights for the first layer in the neural network.
        theta2 (array_like): Weights for the second layer in the neural network.
    """
    def new_trained(self,thetas, epislom):
        for i in range(self.num_layers - 1):
            in_size = self.layer_sizes[i]
            out_size = self.layer_sizes[i + 1]
            #https://www.geeksforgeeks.org/python/numpy-random-uniform-in-python/
            #uniform(rango, rango, n) deuvelve una lista de tamaño n en el rango
            theta = np.random.uniform(-epislom, epislom, (out_size, in_size + 1))
            thetas.append(theta)
        
    """
    Num elements in the training data. (private)

    Args:
        x (array_like): input data. 
    """
    def _size(self,x):
        return x.shape[0]
    
    """
    Computes de sigmoid function of z (private)

    Args:
        z (array_like): activation signal received by the layer.
    """
    def _sigmoid(self,z):
        sig = 1 / (1 + np.exp(-z))
        return sig

    """
    Computes de sigmoid derivation of de activation (private)

    Args:
        a (array_like): activation received by the layer.
    """   
    def _sigmoidPrime(self,a):
        return a * (1 - a)

    """
    Run the feedwordwar neural network step

    Args:
        x (array_like): input of the neural network.

	Return 
	------
    a_list (array_like): activations functions of each layers
    z_list (array_like): signal fuction of A - 1 layers
    z2,z3 (array_like): signal fuction of two last layers
    """
    def feedforward(self,x):
        
        a_list = []
        z_list = []

        a_list.append(np.hstack([np.ones((self._size(x), 1)), x]))  # capa de entrada con bias

        for theta in self.thetas:
            z = a_list[-1] @ theta.T    # signal fuction of the layer
            a = self._sigmoid(z)        # activation fuction of the layer
            z_list.append(z)

            # bias a todas menos la última capa
            # en python -1 en una lista accede al ultimo elemento
            # te libras de hacer la lenght - 1!!
            if theta is not self.thetas[-1]:
                a = np.hstack([np.ones((self._size(a), 1)), a])
            a_list.append(a)

        return a_list, z_list



    """
    Computes only the cost of a previously generated output (private)

    Args:
        yPrime (array_like): output generated by neural network.
        y (array_like): output from the dataset
        lambda_ (scalar): regularization parameter

	Return 
	------
	J (scalar): the cost.
    """
    def compute_cost(self, yPrime, y, lambda_):
        m = y.shape[0]  
        J = (-1 / m) * np.sum(y * np.log(yPrime) + (1 - y) * np.log(1 - yPrime))
        return J + self._regularizationL2Cost(m, lambda_)
    

    """
    Get the class with highest activation value

    Args:
        a3 (array_like): output generated by neural network.

	Return 
	------
	p (scalar): the class index with the highest activation value.
    """
    def predict(self,a3):
        p = np.argmax(a3, axis=1)
        return p
    

    """
    Compute the gradients of both theta matrix parámeters and cost J

    Args:
        x (array_like): input of the neural network.
        y (array_like): output of the neural network.
        lambda_ (scalar): regularization.

	Return 
	------
	J: cost
    grad1, grad2: the gradient matrix (same shape than theta1 and theta2)
    """
    def compute_gradients(self, x, y, lambda_):
        m = x.shape[0]
        a_list, z_list = self.feedforward(x)

        # Antes lo haciamos pasandole a3 que era la última capa, 
        # ahora la última capa es a_list[-1]
        J = self.compute_cost(a_list[-1], y, lambda_)   

        # Necesitamos una lista de deltas
        # Hay una delta por cada capa pero como el input no tiene error esa la obviamos
        delta_list = []
        # Guardamos los gradientes
        gradient_list = []

        # Recordatorio de que tenemos self.num_layers para el número de capas totales
        # Tambien tenemos self.layers_sizes que es la lista con todas las capas

        # El delta de la última capa se calcula de manera diferente por lo que lo hacemos a parte
        delta_list += [a_list[-1] - y]    # Error del output

        Delta = delta_list[-1].T @ a_list[-2]  # Cogemos el penultimo elemento
        gradO = Delta / m

        gradO += self._regularizationL2Gradient(self.thetas[-1], lambda_, m)
        gradient_list += [gradO]

        # Para cada capa oculta hacemos lo siguiente
        # Esto se hace por cada capa oculta
        j = len(self.hiddenLayers) - 1  # Restamos 1 porque termina al llegar a 0
        for i in range(j, -1, -1):

            # Algunas cosas tienen que ir en orden creciente
            a = j - i 

            delta =  (delta_list[-1] @ self.thetas[i + 1][:,1:]) * self._sigmoidPrime(self._sigmoid(z_list[a]))

            delta_list += [delta.T @ a_list[a]]
            grad = delta_list[-1] / m

            grad += self._regularizationL2Gradient(self.thetas[i], lambda_, m)
            gradient_list += [grad]

        # Le damos la vuelta a la lista de los gradientes para que vaya de izquierda a derecha
        gradient_list.reverse()

        
        return J, gradient_list
    
    """
    Compute L2 regularization gradient

    Args:
        theta (array_like): a theta matrix to calculate the regularization.
        lambda_ (scalar): regularization.
        m (scalar): the size of the X examples.

	Return 
	------
	L2 Gradient value
    """
    def _regularizationL2Gradient(self, theta, lambda_, m):
        reg = (lambda_ / m) * np.copy(theta)
        reg[:, 0] = 0  # no regularizar bias
        return reg
    
    
    """
    Compute L2 regularization cost

    Args:
        lambda_ (scalar): regularization.
        m (scalar): the size of the X examples.

	Return 
	------
	L2 cost value
    """
    def _regularizationL2Cost(self, m, lambda_):
        # PARA EXCLUIR EL BIAS:
        # diapo 13 tema 3 aprendizaje
        # cost1 = np.sum(self.theta1[:, 1:] ** 2) 
        # cost2 = np.sum(self.theta2[:, 1:] ** 2)
        # reg_cost_final = (lambda_ / (2 * m)) * (cost1 + cost2)
        # return reg_cost_final
        cost = 0
        for theta in self.thetas:
            cost += np.sum(theta[:, 1:] ** 2)
        return (lambda_ / (2 * m)) * cost
    
    
    def backpropagation(self, x, y, alpha, lambda_, numIte, verbose=0):
        Jhistory = []
        for i in range(numIte):
            ##TO-DO: calculate gradients and update both theta matrix
            J, grads = self.compute_gradients(x, y, lambda_)
            # self.theta1 -= alpha * grad1
            # self.theta2 -= alpha * grad2
            for l in range(len(self.thetas)):
                self.thetas[l] -= alpha * grads[l]
            Jhistory.append(J)
            if verbose > 0 :
                if i % verbose == 0 or i == (numIte-1):
                    print(f"Iteration {(i+1):6}: Cost {float(J):8.4f}   ")
        return Jhistory
    


"""
target_gradient funcitón of gradient test 1
"""
# def target_gradient(input_layer_size,hidden_layer_size,num_labels,x,y,reg_param):
#     mlp = MLP(input_layer_size,hidden_layer_size,num_labels)
#     J, grad1, grad2 = mlp.compute_gradients(x,y,reg_param)
#     return J, grad1, grad2, mlp.theta1, mlp.theta2


"""
costNN funcitón of gradient test 1
"""
# def costNN(Theta1, Theta2,x, ys, reg_param):
#     mlp = MLP(x.shape[1],1, ys.shape[1])
#     mlp.new_trained(Theta1,Theta2)
#     J, grad1, grad2 = mlp.compute_gradients(x,ys,reg_param)
#     return J, grad1, grad2


"""
mlp_backprop_predict 2 to be execute test 2
"""
# def MLP_backprop_predict(X_train,y_train, X_test, alpha, lambda_, num_ite, verbose):
#     mlp = MLP(X_train.shape[1],25,y_train.shape[1])
#     Jhistory = mlp.backpropagation(X_train,y_train,alpha,lambda_,num_ite,verbose)
#     a3= mlp.feedforward(X_test)[2]
#     y_pred=mlp.predict(a3)
#     return y_pred